---
phase: 03-data-extraction
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/lib/db/schema.ts
  - src/lib/extraction/fallback-chain.ts
  - src/app/api/extract/route.ts
  - next.config.ts
autonomous: true

must_haves:
  truths:
    - "Companies table stores extracted company data"
    - "Extraction follows fallback chain: scrape -> enrich -> partial"
    - "System never returns empty result - always has data or missing fields list"
    - "API endpoint accepts URL and returns extraction result"
  artifacts:
    - path: "src/lib/db/schema.ts"
      provides: "Companies table schema"
      contains: "companies = pgTable"
    - path: "src/lib/extraction/fallback-chain.ts"
      provides: "Extraction orchestrator with fallback logic"
      exports: ["extractCompanyData", "ExtractionResult"]
    - path: "src/app/api/extract/route.ts"
      provides: "POST endpoint for extraction"
      exports: ["POST"]
  key_links:
    - from: "src/lib/extraction/fallback-chain.ts"
      to: "src/lib/extraction/scraper.ts"
      via: "calls fetchHtml and extractCompanyInfo"
      pattern: "fetchHtml|extractCompanyInfo"
    - from: "src/lib/extraction/fallback-chain.ts"
      to: "src/lib/extraction/enrichment.ts"
      via: "calls enrichCompany as fallback"
      pattern: "enrichCompany"
    - from: "src/app/api/extract/route.ts"
      to: "src/lib/extraction/fallback-chain.ts"
      via: "calls extractCompanyData"
      pattern: "extractCompanyData"
---

<objective>
Create the companies database table, extraction fallback chain orchestrator, and API endpoint.

Purpose: The fallback chain is the core logic ensuring users always get actionable data. It coordinates scraping, enrichment, and partial data assembly.

Output: Working /api/extract endpoint that accepts a URL and returns extracted company data (or partial data with missing fields).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-data-extraction/03-RESEARCH.md
@.planning/phases/03-data-extraction/03-CONTEXT.md

# Prior plan output
@.planning/phases/03-data-extraction/03-01-SUMMARY.md

# Existing infrastructure
@src/lib/db/schema.ts
@src/lib/validations/company.ts
@src/lib/extraction/scraper.ts
@src/lib/extraction/enrichment.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Companies Database Table</name>
  <files>src/lib/db/schema.ts</files>
  <action>
Add companies table to existing schema file:

```typescript
export const companies = pgTable('companies', {
  id: uuid('id').primaryKey().defaultRandom(),
  userId: uuid('user_id')
    .notNull()
    .references(() => profiles.id, { onDelete: 'cascade' }),

  // Identifiers
  domain: text('domain').notNull(),
  url: text('url').notNull(),

  // Core data
  name: text('name').notNull(),
  description: text('description'),
  industry: text('industry'),
  employeeCount: text('employee_count'),
  location: text('location'),
  foundedYear: text('founded_year'),

  // Arrays as JSONB
  techStack: jsonb('tech_stack').$type<string[]>().default([]),
  emails: jsonb('emails').$type<string[]>().default([]),
  phones: jsonb('phones').$type<string[]>().default([]),

  // Social/meta
  linkedIn: text('linkedin'),
  twitter: text('twitter'),
  logoUrl: text('logo_url'),

  // Extraction metadata
  extractionSources: jsonb('extraction_sources').$type<string[]>().default([]),
  extractionConfidence: text('extraction_confidence'), // high/medium/low

  // Timestamps
  createdAt: timestamp('created_at', { withTimezone: true }).defaultNow().notNull(),
  updatedAt: timestamp('updated_at', { withTimezone: true }).defaultNow().notNull(),
})
```

Export types:
- `Company = typeof companies.$inferSelect`
- `NewCompany = typeof companies.$inferInsert`

Note: The migration will be created later when database sync is needed.
  </action>
  <verify>
Run `npm run build` to verify schema compiles.
Run `npm run db:generate` to verify Drizzle can generate migration.
  </verify>
  <done>
Companies table defined in schema, types exported, Drizzle generates migration without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fallback Chain Orchestrator</name>
  <files>src/lib/extraction/fallback-chain.ts</files>
  <action>
Create the core extraction orchestrator that implements the fallback chain pattern:

1. Define `ExtractionResult` interface:
```typescript
export interface ExtractionResult {
  data: PartialCompanyData;
  sources: ('scrape' | 'enrichment' | 'manual')[];
  confidence: 'high' | 'medium' | 'low';
  needsManualInput: boolean;
  missingFields: string[];
}
```

2. Implement `extractCompanyData(url: string): Promise<ExtractionResult>`:

```typescript
export async function extractCompanyData(url: string): Promise<ExtractionResult> {
  // Parse domain from URL
  const domain = new URL(url).hostname.replace('www.', '');
  let data: PartialCompanyData = { domain };
  const sources: ExtractionResult['sources'] = [];

  // Tier 1: Try scraping first
  try {
    const html = await fetchHtml(url);
    const $ = cheerio.load(html);
    const scraped = extractCompanyInfo($, url);
    const contacts = extractContactInfo($);

    if (scraped) {
      data = { ...data, ...scraped, ...contacts };
      sources.push('scrape');
    }
  } catch (error) {
    console.error('Scraping failed:', error instanceof Error ? error.message : error);
    // Continue to fallback - don't rethrow
  }

  // Tier 2: Enrich with API if scraping incomplete
  const missingAfterScrape = getMissingFields(data);
  if (missingAfterScrape.length > 0) {
    try {
      const enriched = await enrichCompany(domain);
      if (enriched) {
        // Only fill missing fields, don't overwrite scraped data
        for (const field of missingAfterScrape) {
          const key = field as keyof typeof enriched;
          if (enriched[key] !== undefined) {
            (data as Record<string, unknown>)[key] = enriched[key];
          }
        }
        sources.push('enrichment');
      }
    } catch (error) {
      console.error('Enrichment failed:', error instanceof Error ? error.message : error);
      // Continue with partial data - don't rethrow
    }
  }

  // Calculate final state
  const missingFields = getMissingFields(data);
  const needsManualInput = missingFields.length > 0;
  const confidence = calculateConfidence(data, sources);

  return {
    data,
    sources,
    confidence,
    needsManualInput,
    missingFields,
  };
}
```

3. Implement helper functions:

```typescript
// Core fields that should be filled for a complete extraction
const CORE_FIELDS = ['name', 'industry', 'description'] as const;

function getMissingFields(data: PartialCompanyData): string[] {
  return CORE_FIELDS.filter(field => !data[field]);
}

function calculateConfidence(
  data: PartialCompanyData,
  sources: ExtractionResult['sources']
): 'high' | 'medium' | 'low' {
  const filledFields = Object.keys(data).filter(
    k => data[k as keyof PartialCompanyData] !== undefined &&
         data[k as keyof PartialCompanyData] !== null &&
         data[k as keyof PartialCompanyData] !== ''
  );

  // High: 5+ fields filled AND scraped data (most reliable)
  if (filledFields.length >= 5 && sources.includes('scrape')) return 'high';

  // Medium: 3+ fields filled
  if (filledFields.length >= 3) return 'medium';

  // Low: Less than 3 fields
  return 'low';
}
```

Import from 03-01 artifacts:
- PartialCompanyData from validations/company
- fetchHtml from extraction/scraper
- extractCompanyInfo, extractContactInfo from extraction/parsers/company-info
- enrichCompany from extraction/enrichment
- cheerio for HTML parsing
  </action>
  <verify>
Run `npm run build` to verify no TypeScript errors.
Create test script that calls extractCompanyData with a test URL and logs the result.
  </verify>
  <done>
Fallback chain orchestrates scrape -> enrich -> partial data flow, never throws, always returns ExtractionResult.
  </done>
</task>

<task type="auto">
  <name>Task 3: Extraction API Endpoint</name>
  <files>src/app/api/extract/route.ts, next.config.ts</files>
  <action>
Create the API route for extraction:

**src/app/api/extract/route.ts:**

```typescript
import { NextRequest, NextResponse } from 'next/server';
import { extractCompanyData } from '@/lib/extraction/fallback-chain';
import { createClient } from '@/lib/supabase/server';

// Vercel Fluid Compute config - allow up to 60s for extraction
export const maxDuration = 60;
export const dynamic = 'force-dynamic';

export async function POST(request: NextRequest) {
  try {
    // Verify user is authenticated
    const supabase = await createClient();
    const { data: { user }, error: authError } = await supabase.auth.getUser();

    if (authError || !user) {
      return NextResponse.json(
        { error: 'Unauthorized' },
        { status: 401 }
      );
    }

    // Parse and validate request body
    const body = await request.json();
    const { url } = body;

    if (!url || typeof url !== 'string') {
      return NextResponse.json(
        { error: 'URL is required' },
        { status: 400 }
      );
    }

    // Validate URL format
    if (!isValidUrl(url)) {
      return NextResponse.json(
        { error: 'Invalid URL format. Please provide a valid http or https URL.' },
        { status: 400 }
      );
    }

    // Run extraction
    const result = await extractCompanyData(url);

    return NextResponse.json(result);
  } catch (error) {
    console.error('Extraction error:', error);

    // Even on error, return something actionable
    return NextResponse.json(
      {
        data: {},
        sources: [],
        confidence: 'low',
        needsManualInput: true,
        missingFields: ['name', 'industry', 'description'],
        error: 'Extraction failed. Please enter company details manually.',
      },
      { status: 200 } // Return 200 since we're providing fallback
    );
  }
}

function isValidUrl(string: string): boolean {
  try {
    const url = new URL(string);
    return url.protocol === 'http:' || url.protocol === 'https:';
  } catch {
    return false;
  }
}
```

**next.config.ts update:**

Add serverExternalPackages for cheerio (if not already present):

```typescript
const nextConfig: NextConfig = {
  // ...existing config...
  serverExternalPackages: ['cheerio'],
};
```

Note: Playwright is NOT added yet - this plan uses Cheerio only. Playwright support is deferred to a future enhancement if static scraping proves insufficient.
  </action>
  <verify>
Run `npm run build` to verify API route compiles.
Run `npm run dev` and test with curl:
```bash
curl -X POST http://localhost:3000/api/extract \
  -H "Content-Type: application/json" \
  -d '{"url": "https://stripe.com"}'
```
Note: Will return 401 without auth cookie - that's expected. Verify the route loads without errors.
  </verify>
  <done>
API endpoint accepts POST with URL, runs extraction, returns ExtractionResult, handles auth and errors.
  </done>
</task>

</tasks>

<verification>
Full extraction pipeline works end-to-end:
- Database schema generates without errors
- Fallback chain orchestrates scrape -> enrich flow
- API endpoint returns extraction results
- Error cases return actionable fallback data
</verification>

<success_criteria>
1. Companies table defined in src/lib/db/schema.ts with all fields
2. Fallback chain in src/lib/extraction/fallback-chain.ts exports extractCompanyData
3. API route at /api/extract accepts POST with URL and returns ExtractionResult
4. Error cases return `needsManualInput: true` with missing fields list
5. `npm run build` passes with no errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-data-extraction/03-02-SUMMARY.md`
</output>
