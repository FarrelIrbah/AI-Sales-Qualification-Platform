---
phase: 03-data-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/validations/company.ts
  - src/lib/extraction/scraper.ts
  - src/lib/extraction/parsers/company-info.ts
  - src/lib/extraction/enrichment.ts
autonomous: true

must_haves:
  truths:
    - "Company data has a defined schema for validation"
    - "System can fetch and parse HTML from company websites"
    - "System can extract company info using JSON-LD, meta tags, and content"
    - "System can enrich company data via Hunter.io API"
  artifacts:
    - path: "src/lib/validations/company.ts"
      provides: "Company data Zod schemas and types"
      exports: ["companyDataSchema", "manualCompanyInputSchema", "CompanyData", "PartialCompanyData"]
    - path: "src/lib/extraction/scraper.ts"
      provides: "HTML fetching and SPA detection"
      exports: ["fetchHtml", "needsJavaScript"]
    - path: "src/lib/extraction/parsers/company-info.ts"
      provides: "HTML parsing to extract company fields"
      exports: ["extractCompanyInfo", "extractContactInfo"]
    - path: "src/lib/extraction/enrichment.ts"
      provides: "Hunter.io API integration"
      exports: ["enrichCompany", "EnrichedCompanyData"]
  key_links:
    - from: "src/lib/extraction/parsers/company-info.ts"
      to: "src/lib/validations/company.ts"
      via: "uses PartialCompanyData type"
      pattern: "PartialCompanyData"
    - from: "src/lib/extraction/enrichment.ts"
      to: "src/lib/validations/company.ts"
      via: "returns EnrichedCompanyData matching schema fields"
      pattern: "EnrichedCompanyData"
---

<objective>
Create the foundational data layer for company extraction: Zod validation schemas, website scraper with Cheerio parsing, and Hunter.io enrichment API client.

Purpose: Establish the data extraction building blocks that the fallback chain will orchestrate. Each component operates independently and returns partial data.

Output: Three independent libraries (validation, scraping, enrichment) that can be composed into the extraction pipeline.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-data-extraction/03-RESEARCH.md
@.planning/phases/03-data-extraction/03-CONTEXT.md

# Existing infrastructure
@src/lib/db/schema.ts
@src/lib/validations/icp.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Company Data Schemas</name>
  <files>src/lib/validations/company.ts</files>
  <action>
Create Zod validation schemas for company data extraction:

1. `companyDataSchema` - Full company data (all fields):
   - name: string().min(1) (required)
   - domain: string().min(1) (required)
   - description: string().optional()
   - industry: string().optional()
   - employeeCount: string().optional() (e.g., "1-10", "11-50", "51-200")
   - location: string().optional()
   - foundedYear: string().optional()
   - techStack: array(string()).default([])
   - emails: array(string().email()).default([])
   - phones: array(string()).default([])
   - linkedIn: string().url().optional()
   - twitter: string().optional()
   - logoUrl: string().url().optional()

2. `partialCompanyDataSchema` - All fields optional except domain (for extraction progress)

3. `manualCompanyInputSchema` - Core fields for manual entry:
   - name: string().min(1, 'Company name is required')
   - industry: string().min(1, 'Industry is required')
   - description: string().optional()
   - employeeCount: string().optional()
   - location: string().optional()

Export types:
- CompanyData = z.infer<typeof companyDataSchema>
- PartialCompanyData = Partial<CompanyData>
- ManualCompanyInput = z.infer<typeof manualCompanyInputSchema>

Use .describe() on fields that may be used for AI extraction in future.
  </action>
  <verify>
Run `npx tsc --noEmit` to verify types compile.
Create test script that validates sample company data against schemas.
  </verify>
  <done>
Schemas export correctly, sample data validates, types are available for import.
  </done>
</task>

<task type="auto">
  <name>Task 2: Website Scraper Foundation</name>
  <files>src/lib/extraction/scraper.ts, src/lib/extraction/parsers/company-info.ts</files>
  <action>
Create the scraping foundation with Cheerio (no Playwright yet - that comes if needed):

**src/lib/extraction/scraper.ts:**

1. `fetchHtml(url: string): Promise<string>` function:
   - Use native fetch with 10s timeout (AbortSignal.timeout)
   - Set realistic User-Agent header
   - Set Accept headers for HTML
   - Handle non-200 responses by throwing
   - Return raw HTML string

2. `needsJavaScript(html: string): boolean` function:
   - Check for SPA indicators: `id="__next"`, `id="root"`, `ng-app`, `data-reactroot`
   - Check if body text < 100 chars (likely JS-rendered content)
   - Return true if page likely needs browser rendering
   - Note: This is for future Playwright fallback, not implemented in this plan

**src/lib/extraction/parsers/company-info.ts:**

1. Install cheerio: `npm install cheerio`

2. `extractCompanyInfo($: CheerioAPI, url: string): PartialCompanyData` function:
   - Extract name from: og:site_name, application-name meta, title tag, domain
   - Extract description from: og:description, meta description, first p tag
   - Parse JSON-LD script tags for Organization/Corporation type:
     - name, description, industry, foundingDate (year), numberOfEmployees
     - address (format as "city, country")
   - Extract logoUrl from og:image or link[rel="icon"]
   - Always set domain from URL hostname

3. `extractContactInfo($: CheerioAPI): { emails: string[], phones: string[] }` function:
   - Use regex to find emails in body text (filter out example.com)
   - Use regex to find phone numbers in body text
   - Deduplicate and limit to 5 each

Import PartialCompanyData from validations/company.ts.
  </action>
  <verify>
Install cheerio: `npm install cheerio`
Run `npm run build` to verify no TypeScript errors.
Create test script that fetches a known website (e.g., stripe.com/about) and logs extracted data.
  </verify>
  <done>
Scraper fetches HTML, parser extracts company info from HTML, types align with schema.
  </done>
</task>

<task type="auto">
  <name>Task 3: Hunter.io Enrichment Client</name>
  <files>src/lib/extraction/enrichment.ts</files>
  <action>
Create Hunter.io Domain Search API client:

1. Define `EnrichedCompanyData` interface matching Hunter.io response fields:
   - name?: string
   - description?: string
   - industry?: string
   - employeeCount?: string (from employees_count)
   - location?: string (formatted from city + country)
   - techStack?: string[] (from technologies)
   - linkedIn?: string
   - twitter?: string

2. `enrichCompany(domain: string): Promise<EnrichedCompanyData | null>` function:
   - Read HUNTER_API_KEY from process.env
   - If no API key, log warning and return null (graceful degradation)
   - Call: `https://api.hunter.io/v2/companies/find?domain={domain}&api_key={key}`
   - Use 10s timeout (AbortSignal.timeout)
   - Parse response with Zod for safety
   - Handle 402 (credits exhausted) by logging and returning null
   - Handle other errors by logging and returning null
   - Transform Hunter response to EnrichedCompanyData:
     - employees_count -> employeeCount
     - city + country -> location
     - technologies -> techStack
   - Return null if no data in response

3. Create Zod schema for Hunter.io response validation:
   - data: nullable object with optional fields
   - Handle missing fields gracefully

Add HUNTER_API_KEY to .env.local.example with comment about where to get it.
  </action>
  <verify>
Run `npm run build` to verify no TypeScript errors.
If HUNTER_API_KEY is set, test with a known domain (e.g., stripe.com).
If not set, verify function returns null gracefully without throwing.
  </verify>
  <done>
Enrichment client compiles, handles missing API key gracefully, returns enriched data when available.
  </done>
</task>

</tasks>

<verification>
All three components compile and are independently testable:
- `npm run build` passes
- Company schemas validate sample data
- Scraper fetches HTML and extracts basic info
- Enrichment client handles both with/without API key scenarios
</verification>

<success_criteria>
1. src/lib/validations/company.ts exports companyDataSchema, manualCompanyInputSchema, and types
2. src/lib/extraction/scraper.ts exports fetchHtml and needsJavaScript
3. src/lib/extraction/parsers/company-info.ts exports extractCompanyInfo and extractContactInfo
4. src/lib/extraction/enrichment.ts exports enrichCompany and EnrichedCompanyData
5. All functions handle errors gracefully (return null or empty data, don't throw)
6. `npm run build` passes with no type errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-data-extraction/03-01-SUMMARY.md`
</output>
